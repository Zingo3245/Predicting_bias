{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_bias_df = pickle.load(open(\"I\\'m_Pickle_Rick/news_bias_df.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = porter.PorterStemmer()\n",
    "stopwords = stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = []\n",
    "    for post in text:\n",
    "        cleaned_words = []\n",
    "        for word in post.split():\n",
    "            low_word = stemmer.stem(word.lower())\n",
    "            if low_word not in stopwords:\n",
    "                cleaned_words.append(low_word)\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_body = news_bias_df.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(main_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 4), max_df = 0.6)\n",
    "cv_data = count_vectorizer.fit_transform(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_vect = pd.DataFrame(cv_data.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 457234)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = count_vectorizer.transform(cleaned_text).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = matutils.Sparse2Corpus(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"window\" + 0.002*\"amp\" + 0.001*\"adsdiv\" + 0.001*\"function\" + 0.001*\"adtech_call_typ\" + 0.001*\"typeof\" + 0.001*\"amp amp\" + 0.001*\"would\" + 0.001*\"break\" + 0.001*\"adid\"'),\n",
       " (1,\n",
       "  '0.001*\"state\" + 0.001*\"would\" + 0.001*\"news\" + 0.000*\"breitbart\" + 0.000*\"hous\" + 0.000*\"democrat\" + 0.000*\"public\" + 0.000*\"washington\" + 0.000*\"like\" + 0.000*\"it\"'),\n",
       " (2,\n",
       "  '0.001*\"would\" + 0.001*\"hous\" + 0.001*\"republican\" + 0.001*\"state\" + 0.001*\"bill\" + 0.001*\"news\" + 0.001*\"year\" + 0.001*\"work\" + 0.000*\"democrat\" + 0.000*\"percent\"'),\n",
       " (3,\n",
       "  '0.001*\"state\" + 0.000*\"vote\" + 0.000*\"iran\" + 0.000*\"hous\" + 0.000*\"new\" + 0.000*\"republican\" + 0.000*\"bill\" + 0.000*\"may\" + 0.000*\"time\" + 0.000*\"secur\"'),\n",
       " (4,\n",
       "  '0.001*\"state\" + 0.001*\"china\" + 0.001*\"would\" + 0.001*\"iran\" + 0.001*\"trade\" + 0.001*\"american\" + 0.001*\"one\" + 0.000*\"year\" + 0.000*\"it\" + 0.000*\"may\"'),\n",
       " (5,\n",
       "  '0.001*\"hous\" + 0.001*\"would\" + 0.001*\"state\" + 0.001*\"democrat\" + 0.001*\"republican\" + 0.001*\"bill\" + 0.001*\"reuters\" + 0.001*\"senat\" + 0.001*\"washington\" + 0.001*\"campaign\"'),\n",
       " (6,\n",
       "  '0.002*\"whether\" + 0.002*\"don\" + 0.001*\"know\" + 0.001*\"would\" + 0.001*\"state\" + 0.001*\"don know\" + 0.001*\"ani\" + 0.001*\"may\" + 0.001*\"it\" + 0.001*\"hous\"'),\n",
       " (7,\n",
       "  '0.001*\"state\" + 0.001*\"would\" + 0.001*\"new\" + 0.001*\"peopl\" + 0.001*\"campaign\" + 0.001*\"republican\" + 0.001*\"democrat\" + 0.000*\"mcgrath\" + 0.000*\"year\" + 0.000*\"washington\"'),\n",
       " (8,\n",
       "  '0.001*\"republican\" + 0.001*\"democrat\" + 0.001*\"would\" + 0.001*\"hous\" + 0.001*\"it\" + 0.001*\"year\" + 0.001*\"say\" + 0.001*\"senat\" + 0.001*\"campaign\" + 0.001*\"state\"'),\n",
       " (9,\n",
       "  '0.001*\"state\" + 0.001*\"new\" + 0.001*\"may\" + 0.001*\"would\" + 0.001*\"it\" + 0.001*\"time\" + 0.001*\"one\" + 0.001*\"cohen\" + 0.001*\"campaign\" + 0.000*\"democrat\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x10dc84d68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_corpus = lda[corpus]\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer2 = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "\n",
    "X = count_vectorizer2.fit_transform(news_bias_df.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer2 = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "\n",
    "X = count_vectorizer2.fit_transform(news_bias_df.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NickThomas/Desktop/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 10\n",
    "n_iter = 10\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics,\n",
    "                                max_iter=n_iter,\n",
    "                                random_state=42,\n",
    "                               learning_method='online')\n",
    "data = lda.fit_transform(X)\n",
    "data[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        print(\"Topic \", ix+1)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "display_topics(lda,count_vectorizer2.get_feature_names(),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN, SpectralClustering, MeanShift\n",
    "\n",
    "\n",
    "X = StandardScaler().fit_transform(data)\n",
    "db = DBSCAN(eps=0.15, min_samples=3).fit(X)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "plt.figure(dpi=100)\n",
    "show_core = True\n",
    "show_non_core = True\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "    if show_core:\n",
    "        xy = X[class_member_mask & core_samples_mask]\n",
    "        x, y = xy[:,0], xy[:,1]\n",
    "        plt.scatter(x, y, c=col, edgecolors='k',  s=20, linewidths=1.1)\n",
    "    \n",
    "    if show_non_core:\n",
    "        xy = X[class_member_mask & ~core_samples_mask]\n",
    "        x, y = xy[:,0], xy[:,1]\n",
    "        plt.scatter(x, y, c=col, s=20, linewidths=1.1)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
