{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handles the web scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "import pickle\n",
    "\n",
    "#Handles stats and data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy\n",
    "import json\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "#NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import porter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping articles from the Huffington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to request first page of Huffington Post politics section\n",
    "url = 'https://www.huffingtonpost.com/section/politics'\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beautiful Soups the url\n",
    "page = response.text\n",
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of articles\n",
    "article_link = []\n",
    "for article in soup.find_all('a', class_=\"card__link yr-card-headline\"):\n",
    "    link = 'https://www.huffingtonpost.com' + article['href']\n",
    "    article_link.append(link)\n",
    "    #article_link = article_link[5:]\n",
    "    print(article_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to cycle through page with links\n",
    "current_url = 'https://www.huffingtonpost.com/section/politics'\n",
    "next_url    = 'https://www.huffingtonpost.com/section/politics?page=2'\n",
    "\n",
    "def url_generator(page_num):\n",
    "    return 'https://www.huffingtonpost.com/section/politics?page={}'.format(page_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(url):\n",
    "    '''Function that will get each url'''\n",
    "    html = requests.get(url).text\n",
    "    return html\n",
    "\n",
    "def parse_article(html):\n",
    "    '''Will find the title, sub title, and main text of Huffington Post articles'''\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    title = soup.find('h1', class_='headline__title').text\n",
    "    sub_title = soup.find('div', class_='headline__subtitle').text\n",
    "    body = soup.find('div', class_='entry__text js-entry-text yr-entry-text').text\n",
    "\n",
    "    article = {\n",
    "        'title': title,\n",
    "        'sub_title': sub_title,\n",
    "        'body': body,\n",
    "    }\n",
    "    \n",
    "    return article\n",
    "\n",
    "def get_parsed_article_from_link(url):\n",
    "    '''Runs the parse article function on each url'''\n",
    "    return parse_article(get_article(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium will start at the current url and iterate through the following pages\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(current_url)\n",
    "time.sleep(1)\n",
    "pages = [2, 3, 4, 5]\n",
    "more_articles = []\n",
    "for x in pages:\n",
    "    driver.get(url_generator(x))\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    for article in soup.find_all('a', class_=\"card__link yr-card-headline\"):\n",
    "        link = 'https://www.huffingtonpost.com' + article['href']\n",
    "        more_articles.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First pass tries to use requests\n",
    "list_o_articles = []\n",
    "problem_articles = []\n",
    "for text in article_link[5:-12]:\n",
    "    #print(text)\n",
    "    try:\n",
    "        art = get_parsed_article_from_link(text.encode())\n",
    "        print(art)\n",
    "        list_o_articles.append(art)\n",
    "    \n",
    "    except:\n",
    "        print(\"Problem processing url \" + text)\n",
    "        problem = text\n",
    "        problem_articles.append(problem)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium gathers articles that were passed into the problem articles list\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "for x in problem_articles:\n",
    "    driver.get(x)\n",
    "    time.sleep(3)\n",
    "    soupy = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    title = soupy.find('h1', class_='headline__title').text\n",
    "    sub_title = soupy.find('div', class_='headline__subtitle').text\n",
    "    body = soupy.find('div', class_='entry__text js-entry-text yr-entry-text').text\n",
    "\n",
    "    articley = {\n",
    "        'title': title,\n",
    "        'sub_title': sub_title,\n",
    "        'body': body,\n",
    "    }\n",
    "\n",
    "    problem_articles.append(articley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First pass at expanded list of articles\n",
    "more_list_o_articles = []\n",
    "more_problem_articles = []\n",
    "for text in more_articles:\n",
    "    #print(text)\n",
    "    try:\n",
    "        art = get_parsed_article_from_link(text.encode())\n",
    "        print(art)\n",
    "        more_list_o_articles.append(art)\n",
    "    \n",
    "    except:\n",
    "        print(\"Problem processing url \" + text)\n",
    "        problem = text\n",
    "        more_problem_articles.append(problem)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium takes a second pass at rejected request articles\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "for x in more_problem_articles:\n",
    "    driver.get(x)\n",
    "    time.sleep(3)\n",
    "    soupy = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    title = soupy.find('h1', class_='headline__title').text\n",
    "    sub_title = soupy.find('div', class_='headline__subtitle').text\n",
    "    body = soupy.find('div', class_='entry__text js-entry-text yr-entry-text').text\n",
    "\n",
    "    articley = {\n",
    "        'title': title,\n",
    "        'sub_title': sub_title,\n",
    "        'body': body,\n",
    "    }\n",
    "\n",
    "    more_list_o_articles.append(articley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save lists as pickle files\n",
    "with open('list_o_articles.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(list_o_articles, picklefile)\n",
    "with open('more_list_o_articles.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(more_list_o_articles, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping articles from Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requests Reuters politics section\n",
    "url_rueters = 'https://www.reuters.com/politics'\n",
    "response = requests.get(url_rueters)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_links_from_rueters(html):\n",
    "    \"\"\"\n",
    "    Function scrolls through Reuters politics section for more links\n",
    "    \"\"\"\n",
    "    driver.get(html)\n",
    "    time.sleep(3)\n",
    "\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    i = 0\n",
    "    while i < 11:\n",
    "    # Scroll down to bottom\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        i += 1\n",
    "    return list(set(article['href'] for article in soup.find_all('a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets links from Reuters politics section\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "rueters_politics = get_case_links_from_rueters(url_rueters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleans the list of articles from Reuters politics section\n",
    "links = []\n",
    "regex = re.compile(\"https:\\/\\/www.reuters.com\\/article\\/[^']+\")\n",
    "linkstr = re.findall(regex, str(rueters_politics))\n",
    "links.append(linkstr)\n",
    "print(linkstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets rid of the list\n",
    "links_ = []\n",
    "for linklist in links:\n",
    "    for link in linklist:\n",
    "        links_.append(link)\n",
    "for ln in links_:\n",
    "    print(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rueters_article(url):\n",
    "    '''Makes requests for Reuters articles url'''\n",
    "    page = requests.get(url)\n",
    "    html = page.text\n",
    "    return html\n",
    "\n",
    "def parse_rueters_article(html):\n",
    "    '''Takes the title and text from the url'''\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    title = soup.find('h1', class_='headline_2zdFM').text\n",
    "    body = soup.find('div', class_='body_1gnLA').text\n",
    "\n",
    "    article = {\n",
    "        'title': title,\n",
    "        'sub_title': sub_title,\n",
    "        'body': body,\n",
    "    }\n",
    "    \n",
    "    return article\n",
    "\n",
    "def get_parsed_article_from_link(url):\n",
    "    '''Works the parse article function on each url'''\n",
    "    return parse_rueters_article(get_rueters_article(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First pass on Reuters risk of articles\n",
    "rueters_list_o_articles = []\n",
    "rueters_problem_articles = []\n",
    "for text in links_:\n",
    "    try:\n",
    "        art = get_parsed_article_from_link(text.encode())\n",
    "        print(art)\n",
    "        rueters_list_o_articles.append(art)\n",
    "    \n",
    "    except:\n",
    "        print(\"Problem processing url \" + text)\n",
    "        problem = text\n",
    "        rueters_problem_articles.append(problem)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium takes a second pass at Reuters articles\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "for x in links_:\n",
    "    driver.get(x)\n",
    "    time.sleep(3)\n",
    "    soupy = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    title = soupy.find('h1', class_='headline_2zdFM').text\n",
    "    sub_title = 'That percentage has barely budged since last y.'\n",
    "    body = soupy.find('div', class_='body_1gnLA').text\n",
    "\n",
    "    articley = {\n",
    "        'title': title,\n",
    "        'sub_title': sub_title,\n",
    "        'body': body,\n",
    "    }\n",
    "    print(articley)\n",
    "    rueters_list_o_articles.append(articley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves list as a pickle file\n",
    "with open('rueters_list_o_articles.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(rueters_list_o_articles, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping articles from Breitbart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the politics section of Breitbart\n",
    "url_breitbart = 'http://www.breitbart.com/big-government/'\n",
    "response = requests.get(url_breitbart)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_links_from_html(html):\n",
    "    \"\"\"\n",
    "    Beautiful Soup function for getting links from page\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    return list(set(article['href'] for article in soup.find_all('a', class_=\"tumbnail-url\")))\n",
    "\n",
    "def get_case_links_from_page(page_num, driver):\n",
    "    '''Used to iterate through Breitbart pages'''\n",
    "    url = url_generator(page_num)\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    html = driver.page_source\n",
    "    return get_case_links_from_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starts at the first politics page and creates function to continue on\n",
    "current_url = 'http://www.breitbart.com/big-government/'\n",
    "next_url    = 'http://www.breitbart.com/big-government/page/2/'\n",
    "\n",
    "def url_generator(page_num):\n",
    "    return 'http://www.breitbart.com/big-government/page/{}/'.format(page_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets list of links from Breitbart politics page\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(current_url)\n",
    "time.sleep(1)\n",
    "pages = [2, 3, 4, 5]\n",
    "b_article = []\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "really_b_articles = []\n",
    "for article in soup.find_all('a', class_=\"thumbnail-url\"):\n",
    "    link = article['href']\n",
    "    really_b_articles.append(link)\n",
    "    #article_link = article_link[5:]\n",
    "for x in pages:\n",
    "    driver.get(url_generator(x))\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    for article in soup.find_all('a', class_=\"thumbnail-url\"):\n",
    "        link = article['href']\n",
    "        b_article.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_b_article(url):\n",
    "    '''Gets text from url'''\n",
    "    page = requests.get(url)\n",
    "    html = page.text\n",
    "    return html\n",
    "\n",
    "def parse_b_article(html):\n",
    "    '''Gets title and main text from Breitbart page'''\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    title = soup.find('h1').text\n",
    "    body = soup.find('div', class_='entry-content').text\n",
    "\n",
    "    article = {\n",
    "        'title': title,\n",
    "        'sub_title': sub_title,\n",
    "        'body': body,\n",
    "    }\n",
    "    \n",
    "    return article\n",
    "\n",
    "def get_parsed_article_from_link(url):\n",
    "    '''Runs function on each url'''\n",
    "    return parse_b_article(get_b_article(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First pass at Breitbart articles\n",
    "h_list_o_articles = []\n",
    "h_problem_articles = []\n",
    "for text in b_article:\n",
    "    try:\n",
    "        art = get_parsed_article_from_link(text.encode())\n",
    "        print(art)\n",
    "        h_list_o_articles.append(art)\n",
    "    \n",
    "    except:\n",
    "        print(\"Problem processing url \" + text)\n",
    "        problem = text\n",
    "        h_problem_articles.append(problem)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixes url issues\n",
    "for x in really_b_articles[:15]:\n",
    "    y = 'http://www.breitbart.com' + x\n",
    "    really_b_articles.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('h_list_o_articles.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(h_list_o_articles, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First pass at other pages of Breitbart articles\n",
    "really_b_list_o_articles = []\n",
    "really_b_problem_articles = []\n",
    "for text in really_b_articles:\n",
    "    try:\n",
    "        art = get_parsed_article_from_link(text.encode())\n",
    "        print(art)\n",
    "        really_b_list_o_articles.append(art)\n",
    "    \n",
    "    except:\n",
    "        print(\"Problem processing url \" + text)\n",
    "        problem = text\n",
    "        really_b_problem_articles.append(problem)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the pickle file\n",
    "with open('really_b_list_o_articles.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(really_b_list_o_articles, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a data frame for Huffington Post articles\n",
    "huffpo1 = pd.DataFrame.from_dict(list_o_articles)\n",
    "huffpo2 = pd.DataFrame.from_dict(more_list_o_articles)\n",
    "huffpo_df = pd.concat([huffpo1,huffpo2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As a numerical category and string source\n",
    "huffpo_df['topic'] = 1\n",
    "huffpo_df['source'] = 'Huffington Post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEOUL (Reuters) - South Korea’s presidential B...</td>\n",
       "      <td>That percentage has barely budged since last y.</td>\n",
       "      <td>South Korea says release of U.S. prisoners 'po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON (Reuters) - President Donald Trump’...</td>\n",
       "      <td>That percentage has barely budged since last y.</td>\n",
       "      <td>Trump pick for State Department energy job app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOSCOW (Reuters) - Russian President Vladimir ...</td>\n",
       "      <td>That percentage has barely budged since last y.</td>\n",
       "      <td>Russia's Putin deeply concerned at USA leaving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>That percentage has barely budged since last y.</td>\n",
       "      <td>Trump says he will propose new tax cuts prior ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SINGAPORE (Reuters) - Sinopec, Asia’s largest ...</td>\n",
       "      <td>That percentage has barely budged since last y.</td>\n",
       "      <td>China's Sinopec to boost U.S. crude imports to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  SEOUL (Reuters) - South Korea’s presidential B...   \n",
       "1  WASHINGTON (Reuters) - President Donald Trump’...   \n",
       "2  MOSCOW (Reuters) - Russian President Vladimir ...   \n",
       "3  WASHINGTON (Reuters) - U.S. President Donald T...   \n",
       "4  SINGAPORE (Reuters) - Sinopec, Asia’s largest ...   \n",
       "\n",
       "                                         sub_title  \\\n",
       "0  That percentage has barely budged since last y.   \n",
       "1  That percentage has barely budged since last y.   \n",
       "2  That percentage has barely budged since last y.   \n",
       "3  That percentage has barely budged since last y.   \n",
       "4  That percentage has barely budged since last y.   \n",
       "\n",
       "                                               title  \n",
       "0  South Korea says release of U.S. prisoners 'po...  \n",
       "1  Trump pick for State Department energy job app...  \n",
       "2  Russia's Putin deeply concerned at USA leaving...  \n",
       "3  Trump says he will propose new tax cuts prior ...  \n",
       "4  China's Sinopec to boost U.S. crude imports to...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates a data frame for Reuters articles\n",
    "rueters = pd.DataFrame.from_dict(rueters_list_o_articles)\n",
    "rueters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As a numerical category and string source\n",
    "rueters['topic'] = 2\n",
    "rueters['source'] = 'Rueters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a data frame for Breitbart articles\n",
    "breitbart1 = pd.DataFrame.from_dict(really_b_list_o_articles)\n",
    "breitbart2 = pd.DataFrame.from_dict(h_list_o_articles)\n",
    "breitbart_df = pd.concat([breitbart1,breitbart2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As a numerical category and string source\n",
    "breitbart_df['topic'] = 3\n",
    "breitbart_df['source'] = 'Breitbart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines all parts into one data frame\n",
    "news_bias_df= pd.concat([huffpo_df, rueters, breitbart_df])\n",
    "news_bias_df = news_bias_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launches Mongo Client\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "news_bias = client.project4.news_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'catalog', 'companies', 'config', 'events', 'local']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See where in mongo to place the articles\n",
    "client.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create news bias events\n",
    "db = client.events\n",
    "news_bias = db.news_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads data frame into Mongo\n",
    "news_articles = json.loads(news_bias_df.T.to_json()).values()\n",
    "news_bias.insert(news_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "news_bias.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': 'Unions Give $1.3 Billion in Member Dues to Left-Wing Groups',\n",
       "  'count': 2},\n",
       " {'_id': 'Report: Rank-and-File FBI Agents Eager to Blow Whistle on Comey, Holder, Lynch',\n",
       "  'count': 2},\n",
       " {'_id': 'WV Senate Watch: Joe Manchin Internal Poll Claims Lead over Morrisey',\n",
       "  'count': 2},\n",
       " {'_id': 'Pompeo Says Trump Will ‘Walk Away’ if North Korea Deal Fails: ‘A Bad Deal Is Not an Option’',\n",
       "  'count': 2},\n",
       " {'_id': 'Pentagon Disinvites China from Naval Exercise for Militarizing the South China Sea',\n",
       "  'count': 2},\n",
       " {'_id': 'Judge Rules President Donald Trump Cannot Block Twitter Trolls',\n",
       "  'count': 2},\n",
       " {'_id': 'Muslim Republican Candidate Omar Qudrat: ‘I Am Against Sharia Law’',\n",
       "  'count': 2},\n",
       " {'_id': 'Dave Grohl Wants To Apologize To The World For ‘Massive Jerk’ Trump',\n",
       "  'count': 4},\n",
       " {'_id': 'Delingpole: Global Warming Has Rotted the Brains of the Political Class',\n",
       "  'count': 2},\n",
       " {'_id': 'NFL Bans Kneeling During The National Anthem', 'count': 4},\n",
       " {'_id': 'Philip Roth Once Torched Donald Trump In The Most Literary Way',\n",
       "  'count': 4},\n",
       " {'_id': 'Robert De Niro Bans Trump From Every Nobu Restaurant', 'count': 4},\n",
       " {'_id': 'Bad Lip Reading Reveals What Was Really Said At The Royal Wedding',\n",
       "  'count': 4},\n",
       " {'_id': \"Family of detained American 'grateful' for his release from North Korea\",\n",
       "  'count': 2},\n",
       " {'_id': \"'Late Night' Writer Vows To 'Take Red Hats Back' From Trump Supporters\",\n",
       "  'count': 3},\n",
       " {'_id': 'Pulitzer Prize-Winning Novelist Philip Roth Dead At 85', 'count': 3},\n",
       " {'_id': 'Michelle Obama Reflects On ‘Scary’ Time At Princeton With Sweet Throwback Snap',\n",
       "  'count': 4},\n",
       " {'_id': \"In 'The Handmaid's Tale,' The Road To Gilead Is Paved With Ann Coulter Doppelgangers\",\n",
       "  'count': 3},\n",
       " {'_id': 'Democrats Weren’t Invited To Review Classified Documents On FBI Informant',\n",
       "  'count': 2},\n",
       " {'_id': 'Poll: Claire McCaskill Trails GOP Challengers by 7, 16 Percent',\n",
       "  'count': 2},\n",
       " {'_id': 'Emilia Clarke Introduced Herself To Prince William In A Pretty Embarrassing Way',\n",
       "  'count': 4},\n",
       " {'_id': 'Trevor Noah Reveals The Question He’d Ask Donald Trump That No Else Has Dared To',\n",
       "  'count': 4},\n",
       " {'_id': 'UNL Researcher Found Guilty of Putting Fake Blood on NRA Leader’s Home',\n",
       "  'count': 2},\n",
       " {'_id': 'When You Should Replace Your Bras, According To Lingerie Experts',\n",
       "  'count': 4},\n",
       " {'_id': \"Democrats Weren't Invited To Review Classified Documents On FBI Informant\",\n",
       "  'count': 3}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lists articles that are duplicates\n",
    "list(news_bias.aggregate([{'$group' : {'_id': '$title', 'count': {'$sum': 1}}},\n",
    "    {'$match': {'count': {'$gte': 2}}},\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cursor\n",
    "cursor = news_bias.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads from Mongo\n",
    "news_bias_df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 605 entries, 0 to 604\n",
      "Data columns (total 6 columns):\n",
      "_id          605 non-null object\n",
      "body         605 non-null object\n",
      "source       605 non-null object\n",
      "sub_title    605 non-null object\n",
      "title        605 non-null object\n",
      "topic        605 non-null int64\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 28.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#check the data frame\n",
    "news_bias_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping redundant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dave Grohl Wants To Apologize To The World For ‘Massive Jerk’ Trump                                                 4\n",
       "When You Should Replace Your Bras, According To Lingerie Experts                                                    4\n",
       "Emilia Clarke Introduced Herself To Prince William In A Pretty Embarrassing Way                                     4\n",
       "Bad Lip Reading Reveals What Was Really Said At The Royal Wedding                                                   4\n",
       "Michelle Obama Reflects On ‘Scary’ Time At Princeton With Sweet Throwback Snap                                      4\n",
       "NFL Bans Kneeling During The National Anthem                                                                        4\n",
       "Philip Roth Once Torched Donald Trump In The Most Literary Way                                                      4\n",
       "Trevor Noah Reveals The Question He’d Ask Donald Trump That No Else Has Dared To                                    4\n",
       "Robert De Niro Bans Trump From Every Nobu Restaurant                                                                4\n",
       "In 'The Handmaid's Tale,' The Road To Gilead Is Paved With Ann Coulter Doppelgangers                                3\n",
       "Pulitzer Prize-Winning Novelist Philip Roth Dead At 85                                                              3\n",
       "'Late Night' Writer Vows To 'Take Red Hats Back' From Trump Supporters                                              3\n",
       "Democrats Weren't Invited To Review Classified Documents On FBI Informant                                           3\n",
       "Poll: Claire McCaskill Trails GOP Challengers by 7, 16 Percent                                                      2\n",
       "UNL Researcher Found Guilty of Putting Fake Blood on NRA Leader’s Home                                              2\n",
       "WV Senate Watch: Joe Manchin Internal Poll Claims Lead over Morrisey                                                2\n",
       "Pompeo Says Trump Will ‘Walk Away’ if North Korea Deal Fails: ‘A Bad Deal Is Not an Option’                         2\n",
       "Report: Rank-and-File FBI Agents Eager to Blow Whistle on Comey, Holder, Lynch                                      2\n",
       "Muslim Republican Candidate Omar Qudrat: ‘I Am Against Sharia Law’                                                  2\n",
       "Judge Rules President Donald Trump Cannot Block Twitter Trolls                                                      2\n",
       "Delingpole: Global Warming Has Rotted the Brains of the Political Class                                             2\n",
       "Pentagon Disinvites China from Naval Exercise for Militarizing the South China Sea                                  2\n",
       "Family of detained American 'grateful' for his release from North Korea                                             2\n",
       "Democrats Weren’t Invited To Review Classified Documents On FBI Informant                                           2\n",
       "Unions Give $1.3 Billion in Member Dues to Left-Wing Groups                                                         2\n",
       "Talking Grievance And Health Care, An Outsider Kentucky Dem Found The Inside Track                                  1\n",
       "Full Text: Mark Zuckerberg’s ‘Apology’ Speech to European Politicians                                               1\n",
       "Three Americans freed by North Korea head home, thank Trump                                                         1\n",
       "Hawkins: It’s Not the AR-15 or the ‘High Capacity’ Magazine, but the Gun-Free Zone                                  1\n",
       "Just One-Third Of Americans Think Trump Is Successfully Draining The Swamp                                          1\n",
       "                                                                                                                   ..\n",
       "Key senators: 'no doubt' Russia sought to interfere in U.S. election                                                1\n",
       "Exclusive — Revolt Underway: Rep. Paul Gosar Calls for Paul Ryan Removal as Speaker, Replacement with Jim Jordan    1\n",
       "Who Is Stefan Halper? Meet the ‘FBI Informant’ Inside Trump’s 2016 Campaign                                         1\n",
       "Mueller told Trump team he would not indict Trump: Giuliani told CNN                                                1\n",
       "Trump's Iran move reminds some of run-up to Iraq war                                                                1\n",
       "Trump says there is no deal with China to lift ban on ZTE Corp                                                      1\n",
       "Donald Trump Hails First Female CIA Director Gina Haspel: ‘She Will Never, Ever Back Down’                          1\n",
       "Exclusive — Roger Stone to Trump: ‘Pick up the Phone and Order Jeff Sessions to Appoint Andrew McCarthy’            1\n",
       "AT&T payments to Trump lawyer more than reported: source                                                            1\n",
       "Democrats urge Trump to withdraw Justice Department nominee tied to Russian bank                                    1\n",
       "U.S. House starts recess as immigration battle rages                                                                1\n",
       "Homeland Security chief Nielsen was close to resigning: New York Times                                              1\n",
       "European Court Estimates Over 90 Million EU Residents Are First- or Second-Generation Migrants                      1\n",
       "Obama administration too slow to probe Russian meddling in 2016: Senate sources                                     1\n",
       "Marco Rubio: China ‘Out-Negotiating’ Trump Administration on Trade                                                  1\n",
       "Germany, Russian foreign ministers call for Iran deal to be upheld                                                  1\n",
       "U.S., Chinese officials to meet Friday, discuss Liu visit: U.S. official                                            1\n",
       "Pompeo reaches out to European counterparts amid Iran tensions                                                      1\n",
       "Trump CIA nominee secures key Democrat's vote, heads for confirmation                                               1\n",
       "Iran can't be threatened militarily: defense minister                                                               1\n",
       "Mick Mulvaney Backs Plan to Oust Paul Ryan, Force Dems to Vote for Nancy Pelosi                                     1\n",
       "Trump and South Korea’s Moon Discuss North Korea in Phone Conversation                                              1\n",
       "Trump Misspells Melania's Name In Welcome Home Tweet, Twitter Roars                                                 1\n",
       "Most Clinton Voters Now Say ‘People Like Them’ Were Better Off 50 Years Ago                                         1\n",
       "Giuliani Says Trump Lawyer Michael Cohen ‘Has No Incriminating Information About The President’                     1\n",
       "Bob Corker Turns Down Offer to Be Ambassador to Australia                                                           1\n",
       "Eric Holder: ‘No Basis’ for Trump to Investigate FBI Election Meddling                                              1\n",
       "FBI ‘Spy’ Stefan Halper Wanted a Job in Donald Trump’s Administration                                               1\n",
       "Stephanie Kelton Has The Biggest Idea In Washington                                                                 1\n",
       "Vietnam Is A Test For What Happens When The U.S. Abandons Climate Diplomacy                                         1\n",
       "Name: title, Length: 558, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a list of duplicate titles\n",
    "news_bias_df.title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what cases the article appears in\n",
    "huh = news_bias_df['title'] == \"Delingpole: Global Warming Has Rotted the Brains of the Political Class\"\n",
    "wha = news_bias_dff[huh]\n",
    "wha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the redundant cases and repeate step 1\n",
    "label = [597]\n",
    "news_bias_df = news_bias_df.drop(labels=label,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the pickle file\n",
    "with open('news_bias_df.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(news_bias_df, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Readies for stop words and stem words\n",
    "stemmer = porter.PorterStemmer()\n",
    "stopwords = stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Removes stop words and changes word to stem words'''\n",
    "    cleaned_text = []\n",
    "    for post in text:\n",
    "        cleaned_words = []\n",
    "        for word in post.split():\n",
    "            low_word = stemmer.stem(word.lower())\n",
    "            if low_word not in stopwords:\n",
    "                cleaned_words.append(low_word)\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_body = news_bias_df.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleans the text\n",
    "cleaned_text = clean_text(main_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Create a matrix of word counts\n",
    "count_vectorizer2 = CountVectorizer(ngram_range=(1, 4),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "\n",
    "X = count_vectorizer2.fit_transform(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#Fit an LDA model\n",
    "n_topics = 3\n",
    "n_iter = 10\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics,\n",
    "                                max_iter=n_iter,\n",
    "                                random_state=42,\n",
    "                               learning_method='online')\n",
    "X_centered_projected = lda.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    '''Creates a list of words in each topics'''\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        print(\"Topic \", ix+1)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "display_topics(lda,count_vectorizer2.get_feature_names(),120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Creates a list of silhouette scores for KMeans\n",
    "SSEs = []\n",
    "Sil_coefs = []\n",
    "for k in range(2,15):\n",
    "    km = KMeans(n_clusters=k, random_state=1)\n",
    "    km.fit(X_centered_projected)\n",
    "    labels = km.labels_\n",
    "    Sil_coefs.append(silhouette_score(X_centered_projected, labels, metric='euclidean'))\n",
    "    SSEs.append(km.inertia_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphs to show the silhouette scores\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5), sharex=True, dpi=200)\n",
    "k_clusters = range(2,15)\n",
    "ax1.plot(k_clusters, Sil_coefs)\n",
    "ax1.set_xlabel('number of clusters')\n",
    "ax1.set_ylabel('silhouette coefficient')\n",
    "\n",
    "ax2.plot(k_clusters, SSEs)\n",
    "ax2.set_xlabel('number of clusters')\n",
    "ax2.set_ylabel('SSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit KMeans\n",
    "km = KMeans(n_clusters=3)\n",
    "clusts = km.fit_predict(X_centered_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot two of the dimenisons\n",
    "plt.scatter(X_centered_projected[:, 0], X_centered_projected[:, 1], c=clusts, cmap=plt.cm.rainbow, alpha=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "#Fits t-SNE for visualization\n",
    "tsne = TSNE(n_components=2)\n",
    "X_transformed = tsne.fit_transform(X_centered_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphs t-SNE\n",
    "plt.figure(dpi=100)\n",
    "\n",
    "cmap = plt.cm.get_cmap('rainbow', 10)\n",
    "plt.scatter(X_transformed[:, 0], X_transformed[:,1], c=clusts, cmap=cmap, alpha = 0.15)\n",
    "#plt.xlim(-20,30)\n",
    "#plt.ylim(-30,-10)\n",
    "plt.colorbar()\n",
    "plt.clim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds a column for all of the cluster\n",
    "news_bias_df['topic_model'] = clusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of cases in each cluster\n",
    "news_bias_df.topic_model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a data frame for quick reference\n",
    "reference = news_bias_df[['source','topic_model']].pivot_table(index='source', columns='topic_model', aggfunc=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>topic_model</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>48</td>\n",
       "      <td>79</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Huffington Post</th>\n",
       "      <td>33</td>\n",
       "      <td>56</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rueters</th>\n",
       "      <td>120</td>\n",
       "      <td>89</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "topic_model        0   1   2\n",
       "source                      \n",
       "Breitbart         48  79  53\n",
       "Huffington Post   33  56  62\n",
       "Rueters          120  89  10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of each publications articles in each topic\n",
    "Breitbart_0 = 48\n",
    "Breitbart_1 = 79\n",
    "Breitbart_2 = 53\n",
    "Huffington_Post_0 = 33\n",
    "Huffington_Post_1 = 56\n",
    "Huffington_Post_2 = 62\n",
    "Rueters_0 = 120\n",
    "Rueters_1 = 89\n",
    "Rueters_2 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Breitbart articles: 180\n",
      "Total Huffington Post articles: 151\n",
      "Total Rueters articles: 219\n",
      "Total topic 0 articles: 201\n",
      "Total topic 1 articles: 224\n",
      "Total topic 2 articles: 125\n"
     ]
    }
   ],
   "source": [
    "#Creates the totals\n",
    "Breitbart = (Breitbart_0 + Breitbart_1 + Breitbart_2)\n",
    "Huffington_Post = (Huffington_Post_0 + Huffington_Post_1 + Huffington_Post_2)\n",
    "Rueters = (Rueters_0 + Rueters_1 + Rueters_2)\n",
    "topic_0 = (Breitbart_0 + Huffington_Post_0 + Rueters_0)\n",
    "topic_1 = (Breitbart_1 + Huffington_Post_1 + Rueters_1)\n",
    "topic_2 = (Breitbart_2 + Huffington_Post_2 + Rueters_2)\n",
    "print(\"Total Breitbart articles:\", Breitbart)\n",
    "print(\"Total Huffington Post articles:\", Huffington_Post)\n",
    "print(\"Total Rueters articles:\", Rueters)\n",
    "print(\"Total topic 0 articles:\", topic_0)\n",
    "print(\"Total topic 1 articles:\", topic_1)\n",
    "print(\"Total topic 2 articles:\", topic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breitbart sum of scores 0.34331274283345176\n",
      "Huffington Post sum of scores 0.3322510625679549\n",
      "Rueters sum of scores 0.49225295976088246\n"
     ]
    }
   ],
   "source": [
    "#Creates the sum of scores to convert all scores to equal 1\n",
    "Breitbart_sum = (((Breitbart_0 / Breitbart) * (Breitbart_0 / topic_0)) + \n",
    "                 ((Breitbart_1 / Breitbart) * (Breitbart_1 / topic_1)) + ((Breitbart_2 / Breitbart) * (Breitbart_2 / topic_2)))\n",
    "Huffington_Post_sum =(((Huffington_Post_0 / Huffington_Post) * (Huffington_Post_0 / topic_0)) + \n",
    "                      ((Huffington_Post_1 / Huffington_Post) * (Huffington_Post_1 / topic_1)) + \n",
    "                      ((Huffington_Post_2 / Huffington_Post) * (Huffington_Post_2 / topic_2)))\n",
    "Rueters_sum = (((Rueters_0 / Rueters) * (Rueters_0 / topic_0)) + \n",
    "                ((Rueters_1 / Rueters) * (Rueters_1 / topic_1)) + \n",
    "                ((Rueters_2 / Rueters) * (Rueters_2 / topic_2)))\n",
    "print(\"Breitbart sum of scores\", Breitbart_sum)\n",
    "print(\"Huffington Post sum of scores\", Huffington_Post_sum)\n",
    "print(\"Rueters sum of scores\", Rueters_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huffpo left wing score: 0.61\n",
      "Huffpo center score: 0.28\n",
      "Huffpo right wing score: 0.11\n"
     ]
    }
   ],
   "source": [
    "print(\"Huffpo left wing score:\", round((Huffington_Post_2 / Huffington_Post) *  (Huffington_Post_2 / topic_2) / Huffington_Post_sum, 2))\n",
    "print(\"Huffpo right wing score:\", round((Huffington_Post_1 / Huffington_Post)  * (Huffington_Post_1 / topic_1) / Huffington_Post_sum, 2))\n",
    "print(\"Huffpo center score:\", round((Huffington_Post_0 / Huffington_Post) * (Huffington_Post_0 / topic_0) / Huffington_Post_sum, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breitbart left wing score: 0.36\n",
      "Breitbart right wing score: 0.45\n",
      "Breitbart center score: 0.19\n"
     ]
    }
   ],
   "source": [
    "print(\"Breitbart left wing score:\", round((Breitbart_2 / Breitbart) *  (Breitbart_2 / topic_2) / Breitbart_sum, 2))\n",
    "print(\"Breitbart right wing score:\", round((Breitbart_1 / Breitbart) *  (Breitbart_1 / topic_1) / Breitbart_sum, 2))\n",
    "print(\"Breitbart center score:\", round((Breitbart_0 / Breitbart) *  (Breitbart_0 / topic_0) / Breitbart_sum, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rueters left wing score: 0.01\n",
      "Rueters right wing score: 0.33\n",
      "Rueters center score: 0.66\n"
     ]
    }
   ],
   "source": [
    "print(\"Rueters left wing score:\", round((Rueters_2 / Rueters) *  (Rueters_2 / topic_2) / Rueters_sum, 2))\n",
    "print(\"Rueters right wing score:\", round((Rueters_1 / Rueters) *  (Rueters_1 / topic_1) / Rueters_sum, 2))\n",
    "print(\"Rueters center score:\", round((Rueters_0 / Rueters) *  (Rueters_0 / topic_0) / Rueters_sum, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
